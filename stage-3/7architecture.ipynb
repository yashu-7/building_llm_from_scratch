{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fc68f6b",
   "metadata": {},
   "source": [
    "## Transformers Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b98bdc",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c147c208",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchinfo import summary\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8801c576",
   "metadata": {},
   "source": [
    "### Positional and Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8866e7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, context_len, embed_dim):\n",
    "        super().__init__()\n",
    "        self.pe = torch.zeros(context_len, embed_dim)\n",
    "\n",
    "        position = torch.arange(0, context_len, dtype=torch.float).unsqueeze(1)\n",
    "        # print(position.shape)\n",
    "\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * -math.log(10000.0)/embed_dim)\n",
    "\n",
    "        self.pe[:, 0::2] = torch.sin(position * div_term)\n",
    "\n",
    "        self.pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.pe = self.pe.unsqueeze(0)\n",
    "\n",
    "        # self.pe_matrix = pe\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "        return x\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, embed_dim, vocab_size):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embed_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405c81a1",
   "metadata": {},
   "source": [
    "### MultiHead Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "572dcd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, context_len, n_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.context_len = context_len\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = embed_dim // n_heads\n",
    "\n",
    "        self.K = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n",
    "        self.Q = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n",
    "        self.V = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n",
    "\n",
    "        self.mask = torch.tril(torch.ones(self.context_len, self.context_len))\n",
    "\n",
    "        self.out = torch.nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        K = self.K(x)\n",
    "        Q = self.Q(x)\n",
    "        V = self.V(x)\n",
    "\n",
    "        print(K.shape)\n",
    "        K = K.view(B, T, self.n_heads, self.head_dim).transpose(1,2)\n",
    "        print(K.shape)\n",
    "\n",
    "        Q = Q.view(B, T, self.n_heads, self.head_dim).transpose(1,2)\n",
    "        V = V.view(B, T, self.n_heads, self.head_dim).transpose(1,2)\n",
    "\n",
    "        attn_scores = Q @ K.transpose(-2,-1)\n",
    "        \n",
    "        scaled_scores = attn_scores / math.sqrt(self.head_dim)\n",
    "\n",
    "        masked_scores = scaled_scores.masked_fill(self.mask[:T, :T]==0, float('-inf'))\n",
    "\n",
    "        attention_weights = F.softmax(masked_scores, dim=-1)\n",
    "\n",
    "        context_vectors = attention_weights @ V\n",
    "        print(context_vectors.shape)\n",
    "\n",
    "        output = context_vectors.transpose(1, 2).contiguous().view(B, T, C)\n",
    "\n",
    "        return self.out(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b982f633",
   "metadata": {},
   "source": [
    "### Decoder class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "07f6315d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, embed_dim, context_len, n_heads):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.context_len = context_len\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        self.attn = MultiHeadAttention(self.embed_dim, self.context_len, self.n_heads)\n",
    "\n",
    "        self.layer_norm1 = nn.LayerNorm(self.embed_dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(self.embed_dim)\n",
    "        \n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(self.embed_dim, 4 * self.embed_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * self.embed_dim, self.embed_dim)\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output = self.attn(x)\n",
    "\n",
    "        x = self.layer_norm1(x + self.dropout(attn_output))\n",
    "\n",
    "        ff_out = self.feedforward(x)\n",
    "        \n",
    "        x = self.layer_norm2(x + self.dropout(ff_out))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40534bf",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "36221325",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderOnlyModel(nn.Module):\n",
    "    def __init__(self, embed_dim, vocab_size, context_len, n_heads, n_layers):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.context_len = context_len\n",
    "        self.n_heads = n_heads\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embeddings = TokenEmbedding(self.embed_dim, self.vocab_size)\n",
    "        self.positional_encoding = PositionalEncoding(self.context_len, self.embed_dim)\n",
    "\n",
    "        self.decoder_blocks = nn.Sequential(*[TransformerDecoder(self.embed_dim, self.context_len, self.n_heads) for _ in range(self.n_layers)])\n",
    "\n",
    "        self.linear = nn.Linear(self.embed_dim, self.vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embed = self.embeddings(x)\n",
    "        pos = self.positional_encoding(embed)\n",
    "        \n",
    "        self.d_block = self.decoder_blocks(pos)\n",
    "        \n",
    "        out = self.linear(self.d_block)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c7d53f",
   "metadata": {},
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f98b9043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 8])\n",
      "\n",
      "Model parameters:\n",
      "DecoderOnlyModel(\n",
      "  (embeddings): TokenEmbedding(\n",
      "    (embedding): Embedding(4, 8)\n",
      "  )\n",
      "  (positional_encoding): PositionalEncoding()\n",
      "  (decoder_blocks): Sequential(\n",
      "    (0): TransformerDecoder(\n",
      "      (attn): MultiHeadAttention(\n",
      "        (K): Linear(in_features=8, out_features=8, bias=False)\n",
      "        (Q): Linear(in_features=8, out_features=8, bias=False)\n",
      "        (V): Linear(in_features=8, out_features=8, bias=False)\n",
      "        (out): Linear(in_features=8, out_features=8, bias=False)\n",
      "      )\n",
      "      (layer_norm1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "      (layer_norm2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "      (feedforward): Sequential(\n",
      "        (0): Linear(in_features=8, out_features=32, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=32, out_features=8, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.5, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(in_features=8, out_features=4, bias=True)\n",
      ")\n",
      "\n",
      "torch.Size([1, 8, 8])\n",
      "torch.Size([1, 2, 8, 4])\n",
      "torch.Size([1, 2, 8, 4])\n",
      "Output logits shape: torch.Size([1, 8, 4])\n",
      "Output logits:\n",
      "tensor([[[ 0.0907,  0.7210, -0.7618,  0.4185],\n",
      "         [ 0.4578, -0.4347,  0.0939, -0.4461],\n",
      "         [-0.5041,  0.6966, -0.3678, -0.7817],\n",
      "         [-0.4503, -0.3542,  0.5255, -0.5365],\n",
      "         [-0.4112,  0.3980,  0.3168, -0.5883],\n",
      "         [ 0.0527,  0.4314,  0.2056, -0.5645],\n",
      "         [ 0.0661, -0.4692,  0.2037, -0.3814],\n",
      "         [ 0.5832,  1.5735, -0.7845,  0.2872]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 8\n",
    "vocab_size = 4\n",
    "context_len = 8  # <<< THE FIX: Set context_len to match the data length\n",
    "n_heads = 2\n",
    "n_layers = 1\n",
    "\n",
    "# Create input data with sequence length = 8\n",
    "input_indices = torch.randint(0, vocab_size, (1, context_len))\n",
    "print(f\"Input shape: {input_indices.shape}\\n\")\n",
    "\n",
    "# Create a model built to handle sequences up to length 8\n",
    "model = DecoderOnlyModel(embed_dim, vocab_size, context_len, n_heads, n_layers)\n",
    "print(f\"Model parameters:\\n{model}\\n\") # Using list() to print them\n",
    "\n",
    "# This will now run without error\n",
    "logits = model(input_indices)\n",
    "print(f\"Output logits shape: {logits.shape}\")\n",
    "print(f\"Output logits:\\n{logits}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b0523316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "DecoderOnlyModel                         [6, 1024, 100000]         --\n",
       "├─TokenEmbedding: 1-1                    [6, 1024, 512]            --\n",
       "│    └─Embedding: 2-1                    [6, 1024, 512]            51,200,000\n",
       "├─PositionalEncoding: 1-2                [6, 1024, 512]            --\n",
       "├─Sequential: 1-3                        [6, 1024, 512]            --\n",
       "│    └─TransformerDecoder: 2-2           [6, 1024, 512]            --\n",
       "│    │    └─MultiHeadAttention: 3-1      [6, 1024, 512]            1,048,576\n",
       "│    │    └─Dropout: 3-2                 [6, 1024, 512]            --\n",
       "│    │    └─LayerNorm: 3-3               [6, 1024, 512]            1,024\n",
       "│    │    └─Sequential: 3-4              [6, 1024, 512]            2,099,712\n",
       "│    │    └─Dropout: 3-5                 [6, 1024, 512]            --\n",
       "│    │    └─LayerNorm: 3-6               [6, 1024, 512]            1,024\n",
       "│    └─TransformerDecoder: 2-3           [6, 1024, 512]            --\n",
       "│    │    └─MultiHeadAttention: 3-7      [6, 1024, 512]            1,048,576\n",
       "│    │    └─Dropout: 3-8                 [6, 1024, 512]            --\n",
       "│    │    └─LayerNorm: 3-9               [6, 1024, 512]            1,024\n",
       "│    │    └─Sequential: 3-10             [6, 1024, 512]            2,099,712\n",
       "│    │    └─Dropout: 3-11                [6, 1024, 512]            --\n",
       "│    │    └─LayerNorm: 3-12              [6, 1024, 512]            1,024\n",
       "│    └─TransformerDecoder: 2-4           [6, 1024, 512]            --\n",
       "│    │    └─MultiHeadAttention: 3-13     [6, 1024, 512]            1,048,576\n",
       "│    │    └─Dropout: 3-14                [6, 1024, 512]            --\n",
       "│    │    └─LayerNorm: 3-15              [6, 1024, 512]            1,024\n",
       "│    │    └─Sequential: 3-16             [6, 1024, 512]            2,099,712\n",
       "│    │    └─Dropout: 3-17                [6, 1024, 512]            --\n",
       "│    │    └─LayerNorm: 3-18              [6, 1024, 512]            1,024\n",
       "│    └─TransformerDecoder: 2-5           [6, 1024, 512]            --\n",
       "│    │    └─MultiHeadAttention: 3-19     [6, 1024, 512]            1,048,576\n",
       "│    │    └─Dropout: 3-20                [6, 1024, 512]            --\n",
       "│    │    └─LayerNorm: 3-21              [6, 1024, 512]            1,024\n",
       "│    │    └─Sequential: 3-22             [6, 1024, 512]            2,099,712\n",
       "│    │    └─Dropout: 3-23                [6, 1024, 512]            --\n",
       "│    │    └─LayerNorm: 3-24              [6, 1024, 512]            1,024\n",
       "│    └─TransformerDecoder: 2-6           [6, 1024, 512]            --\n",
       "│    │    └─MultiHeadAttention: 3-25     [6, 1024, 512]            1,048,576\n",
       "│    │    └─Dropout: 3-26                [6, 1024, 512]            --\n",
       "│    │    └─LayerNorm: 3-27              [6, 1024, 512]            1,024\n",
       "│    │    └─Sequential: 3-28             [6, 1024, 512]            2,099,712\n",
       "│    │    └─Dropout: 3-29                [6, 1024, 512]            --\n",
       "│    │    └─LayerNorm: 3-30              [6, 1024, 512]            1,024\n",
       "│    └─TransformerDecoder: 2-7           [6, 1024, 512]            --\n",
       "│    │    └─MultiHeadAttention: 3-31     [6, 1024, 512]            1,048,576\n",
       "│    │    └─Dropout: 3-32                [6, 1024, 512]            --\n",
       "│    │    └─LayerNorm: 3-33              [6, 1024, 512]            1,024\n",
       "│    │    └─Sequential: 3-34             [6, 1024, 512]            2,099,712\n",
       "│    │    └─Dropout: 3-35                [6, 1024, 512]            --\n",
       "│    │    └─LayerNorm: 3-36              [6, 1024, 512]            1,024\n",
       "│    └─TransformerDecoder: 2-8           [6, 1024, 512]            --\n",
       "│    │    └─MultiHeadAttention: 3-37     [6, 1024, 512]            1,048,576\n",
       "│    │    └─Dropout: 3-38                [6, 1024, 512]            --\n",
       "│    │    └─LayerNorm: 3-39              [6, 1024, 512]            1,024\n",
       "│    │    └─Sequential: 3-40             [6, 1024, 512]            2,099,712\n",
       "│    │    └─Dropout: 3-41                [6, 1024, 512]            --\n",
       "│    │    └─LayerNorm: 3-42              [6, 1024, 512]            1,024\n",
       "│    └─TransformerDecoder: 2-9           [6, 1024, 512]            --\n",
       "│    │    └─MultiHeadAttention: 3-43     [6, 1024, 512]            1,048,576\n",
       "│    │    └─Dropout: 3-44                [6, 1024, 512]            --\n",
       "│    │    └─LayerNorm: 3-45              [6, 1024, 512]            1,024\n",
       "│    │    └─Sequential: 3-46             [6, 1024, 512]            2,099,712\n",
       "│    │    └─Dropout: 3-47                [6, 1024, 512]            --\n",
       "│    │    └─LayerNorm: 3-48              [6, 1024, 512]            1,024\n",
       "├─Linear: 1-4                            [6, 1024, 100000]         51,300,000\n",
       "==========================================================================================\n",
       "Total params: 127,702,688\n",
       "Trainable params: 127,702,688\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 766.22\n",
       "==========================================================================================\n",
       "Input size (MB): 0.05\n",
       "Forward/backward pass size (MB): 7154.96\n",
       "Params size (MB): 510.81\n",
       "Estimated Total Size (MB): 7665.82\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from torchinfo import summary # Import torchinfo\n",
    "\n",
    "# --- CORRECTED CUSTOM MODULES ---\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, embed_dim, vocab_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, context_len, embed_dim):\n",
    "        super().__init__()\n",
    "        # Ensure the order is (context_len, embed_dim)\n",
    "        pe = torch.zeros(context_len, embed_dim)\n",
    "        position = torch.arange(0, context_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * -math.log(10000.0) / embed_dim)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return x\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, context_len, n_heads):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.context_len = context_len\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = embed_dim // n_heads\n",
    "        self.K = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n",
    "        self.Q = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n",
    "        self.V = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n",
    "        self.register_buffer('mask', torch.tril(torch.ones(context_len, context_len)))\n",
    "        self.out = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        K = self.K(x).view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        Q = self.Q(x).view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        V = self.V(x).view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        attn_scores = (Q @ K.transpose(-2, -1)) * (self.head_dim**-0.5)\n",
    "        masked_scores = attn_scores.masked_fill(self.mask[:T, :T] == 0, float('-inf'))\n",
    "        attention_weights = F.softmax(masked_scores, dim=-1)\n",
    "        output = (attention_weights @ V).transpose(1, 2).contiguous().view(B, T, C)\n",
    "        return self.out(output)\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, embed_dim, context_len, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(embed_dim, context_len, n_heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 4 * embed_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * embed_dim, embed_dim)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output = self.attention(x)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x\n",
    "\n",
    "class DecoderOnlyModel(nn.Module):\n",
    "    def __init__(self, embed_dim, vocab_size, context_len, n_heads, n_layers):\n",
    "        super().__init__()\n",
    "        self.token_embedding = TokenEmbedding(embed_dim, vocab_size)\n",
    "        self.positional_encoding = PositionalEncoding(context_len, embed_dim)\n",
    "        self.decoder_blocks = nn.Sequential(*[TransformerDecoder(embed_dim, context_len, n_heads) for _ in range(n_layers)])\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.token_embedding(x)\n",
    "        x = self.positional_encoding(x)\n",
    "        x = self.decoder_blocks(x)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# --- TEST SCRIPT ---\n",
    "\n",
    "embed_dim = 512\n",
    "vocab_size = 100_000\n",
    "context_len = 1024\n",
    "n_heads = 16\n",
    "n_layers = 8\n",
    "\n",
    "model = DecoderOnlyModel(embed_dim, vocab_size, context_len, n_heads, n_layers)\n",
    "\n",
    "batch_size = 6\n",
    "input_shape = (batch_size, context_len)\n",
    "\n",
    "# This will now run without error and give you the summary.\n",
    "summary(model, input_size=input_shape, dtypes=[torch.long], depth=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
