{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a662fa2",
   "metadata": {},
   "source": [
    "## Masked Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603f6b20",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5eb253ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087e034f",
   "metadata": {},
   "source": [
    "### Masked Attention Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d1916b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedAttention(torch.nn.Module):\n",
    "    def __init__(self, embed_dim, context_len):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.context_len = context_len\n",
    "        \n",
    "        self.K = torch.nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n",
    "        self.Q = torch.nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n",
    "        self.V = torch.nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n",
    "\n",
    "        self.mask = torch.tril(torch.ones(self.context_len, self.context_len))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "\n",
    "        Key = self.K(x)\n",
    "        Query = self.Q(x)\n",
    "        Value = self.V(x)\n",
    "\n",
    "        print(Key.shape)\n",
    "        print(Query.shape)\n",
    "\n",
    "        attention_scores = Query@Key.transpose(-2, -1)\n",
    "        print(attention_scores)\n",
    "        print(attention_scores.shape)\n",
    "\n",
    "        scaled_attn_scores = attention_scores/math.sqrt(Key.size(-1))\n",
    "        print(scaled_attn_scores)\n",
    "        print(scaled_attn_scores.shape)\n",
    "\n",
    "        masked_scaled_attn_scores = scaled_attn_scores.masked_fill(self.mask[:T, :T] == 0, float('-inf'))\n",
    "        print(masked_scaled_attn_scores)\n",
    "        print(masked_scaled_attn_scores.shape)\n",
    "\n",
    "        normalized_masked_scaled_attn_scores = F.softmax(masked_scaled_attn_scores, dim=-1)\n",
    "        print(normalized_masked_scaled_attn_scores)\n",
    "        print(normalized_masked_scaled_attn_scores.shape)\n",
    "\n",
    "        context_vectors = normalized_masked_scaled_attn_scores@Value\n",
    "\n",
    "        return context_vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c735eaac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 8])\n",
      "torch.Size([1, 4, 8])\n",
      "tensor([[[-0.0484,  0.0866,  0.0112, -0.0115],\n",
      "         [-0.0723,  0.2632,  0.1180,  0.0911],\n",
      "         [-0.1217,  0.2157,  0.0313,  0.0124],\n",
      "         [-0.0566,  0.4067,  0.1926,  0.2012]]], grad_fn=<UnsafeViewBackward0>)\n",
      "torch.Size([1, 4, 4])\n",
      "tensor([[[-0.0171,  0.0306,  0.0040, -0.0041],\n",
      "         [-0.0256,  0.0930,  0.0417,  0.0322],\n",
      "         [-0.0430,  0.0763,  0.0111,  0.0044],\n",
      "         [-0.0200,  0.1438,  0.0681,  0.0711]]], grad_fn=<DivBackward0>)\n",
      "torch.Size([1, 4, 4])\n",
      "tensor([[[-0.0171,    -inf,    -inf,    -inf],\n",
      "         [-0.0256,  0.0930,    -inf,    -inf],\n",
      "         [-0.0430,  0.0763,  0.0111,    -inf],\n",
      "         [-0.0200,  0.1438,  0.0681,  0.0711]]], grad_fn=<MaskedFillBackward0>)\n",
      "torch.Size([1, 4, 4])\n",
      "tensor([[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.4704, 0.5296, 0.0000, 0.0000],\n",
      "         [0.3142, 0.3540, 0.3317, 0.0000],\n",
      "         [0.2291, 0.2698, 0.2502, 0.2509]]], grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([1, 4, 4])\n",
      "tensor([[[-0.2735, -0.0113,  0.1154,  0.1179, -0.1543,  0.0779, -0.0080,\n",
      "           0.0384],\n",
      "         [-0.1314, -0.0465,  0.1555, -0.0492, -0.1871,  0.2637,  0.1044,\n",
      "           0.1089],\n",
      "         [-0.1110, -0.0023,  0.1678, -0.1003, -0.2252,  0.2746,  0.0900,\n",
      "           0.1279],\n",
      "         [-0.1090,  0.0389,  0.1846, -0.1171, -0.2168,  0.2580,  0.0887,\n",
      "           0.1749]]], grad_fn=<UnsafeViewBackward0>)\n",
      "torch.Size([1, 4, 8])\n",
      "torch.Size([1, 4, 8])\n",
      "tensor([[[-0.0484,  0.0866,  0.0112, -0.0115],\n",
      "         [-0.0723,  0.2632,  0.1180,  0.0911],\n",
      "         [-0.1217,  0.2157,  0.0313,  0.0124],\n",
      "         [-0.0566,  0.4067,  0.1926,  0.2012]]], grad_fn=<UnsafeViewBackward0>)\n",
      "torch.Size([1, 4, 4])\n",
      "tensor([[[-0.0171,  0.0306,  0.0040, -0.0041],\n",
      "         [-0.0256,  0.0930,  0.0417,  0.0322],\n",
      "         [-0.0430,  0.0763,  0.0111,  0.0044],\n",
      "         [-0.0200,  0.1438,  0.0681,  0.0711]]], grad_fn=<DivBackward0>)\n",
      "torch.Size([1, 4, 4])\n",
      "tensor([[[-0.0171,    -inf,    -inf,    -inf],\n",
      "         [-0.0256,  0.0930,    -inf,    -inf],\n",
      "         [-0.0430,  0.0763,  0.0111,    -inf],\n",
      "         [-0.0200,  0.1438,  0.0681,  0.0711]]], grad_fn=<MaskedFillBackward0>)\n",
      "torch.Size([1, 4, 4])\n",
      "tensor([[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.4704, 0.5296, 0.0000, 0.0000],\n",
      "         [0.3142, 0.3540, 0.3317, 0.0000],\n",
      "         [0.2291, 0.2698, 0.2502, 0.2509]]], grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([1, 4, 4])\n",
      "tensor([[[-0.2735, -0.0113,  0.1154,  0.1179, -0.1543,  0.0779, -0.0080,\n",
      "           0.0384],\n",
      "         [-0.1314, -0.0465,  0.1555, -0.0492, -0.1871,  0.2637,  0.1044,\n",
      "           0.1089],\n",
      "         [-0.1110, -0.0023,  0.1678, -0.1003, -0.2252,  0.2746,  0.0900,\n",
      "           0.1279],\n",
      "         [-0.1090,  0.0389,  0.1846, -0.1171, -0.2168,  0.2580,  0.0887,\n",
      "           0.1749]]], grad_fn=<UnsafeViewBackward0>)\n",
      "torch.Size([1, 4, 8])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand((1, 4, 8), dtype=torch.float)\n",
    "\n",
    "attn = MaskedAttention(8, 4)\n",
    "print(attn.forward(x))\n",
    "context_vectors = attn.forward(x)\n",
    "print(context_vectors)\n",
    "print(context_vectors.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
